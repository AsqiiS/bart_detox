{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a340ba1-ec2f-428a-9e8b-1cb5a2372c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuld\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "        num_rows: 19744\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "detox_dataset = load_dataset('s-nlp/paradetox')\n",
    "detox_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77939bfa-4fdd-4625-b88e-dde5194327d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('cuda') \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2524b05-db01-4c19-b7f3-394dc256f983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "    num_rows: 19744\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639967c2-9ba9-4348-b440-d8544500112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset \n",
    "\n",
    "train_val, test = train_test_split(detox_dataset['train'], test_size=0.2, random_state=42) # 20% data goes to test set, random_state for reproducibility\n",
    "train_val_ds = Dataset.from_dict(train_val)\n",
    "\n",
    "train, val = train_test_split(train_val_ds, test_size=0.125, random_state=42) # 0.125 * 0.8 = 0.1, which means 10% of the data goes to val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f5918f-6a4f-43b7-bd0c-cef740f88b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train dataset:  13820 13820\n",
      "length of validation dataset:  1975 1975\n",
      "length of test dataset:  3949 3949\n"
     ]
    }
   ],
   "source": [
    "print('length of train dataset: ', len(train['en_toxic_comment']), len(train['en_toxic_comment']))\n",
    "print('length of validation dataset: ', len(val['en_toxic_comment']), len(val['en_toxic_comment']))\n",
    "print('length of test dataset: ', len(test['en_toxic_comment']), len(test['en_toxic_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d4f5e7-a031-4cd8-bc10-184adc14a1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('anonymous is so fucking annoying .', 'anonymous is so annoying .')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['en_toxic_comment'][0], test['en_neutral_comment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ddb4bc-d603-42bc-9ae4-d9838dff41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict(train)\n",
    "val_dataset = Dataset.from_dict(val)\n",
    "test_dataset = Dataset.from_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf2186f-9654-4fa4-91cc-3d78b1e2167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "        num_rows: 13820\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "        num_rows: 1975\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "        num_rows: 3949\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "detox_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "detox_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f5ed96-260e-4318-a92a-1bf0794e954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'en_toxic_comment': 'will do haha , need to pay for the fuckin thing first haha',\n",
       "  'en_neutral_comment': 'will do haha , need to pay for the thing first haha'},\n",
       " {'en_toxic_comment': 'that dp fresh as shit tho',\n",
       "  'en_neutral_comment': 'That do fresh as tho'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detox_dataset['train'][0], detox_dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8a808a7c-81f4-4e0e-978a-de315c7f332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from box import Box\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ef02000e-3a5d-40b7-9884-72adaf3a38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "249557ac-c20e-4f4f-9b0b-e615064ce9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "892f33c2-7740-41cf-89a7-a5146efdf27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of toxic sentences:  54.838279983792546\n",
      "maximum length of toxic sentences:  135\n",
      "average length of neutral sentences:  46.2729943273906\n",
      "maximum length of neutral sentences:  149\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'validation', 'test']\n",
    "\n",
    "max_len_toxic, av_len_toxic = 0, 0\n",
    "max_len_neutral, av_len_neutral = 0, 0\n",
    "\n",
    "for split in splits:\n",
    "    for item in detox_dataset[split]:\n",
    "        tox_len = len(item['en_toxic_comment'])\n",
    "        neu_len = len(item['en_neutral_comment'])\n",
    "        \n",
    "        if tox_len > max_len_toxic: \n",
    "            max_len_toxic = tox_len\n",
    "        if neu_len > max_len_neutral:\n",
    "            max_len_neutral = neu_len\n",
    "\n",
    "        av_len_toxic += tox_len\n",
    "        av_len_neutral += neu_len\n",
    "\n",
    "total = len(detox_dataset['train']) + len(detox_dataset['validation']) + len(detox_dataset['test'])\n",
    "\n",
    "av_len_toxic /= total \n",
    "av_len_neutral /= total \n",
    "\n",
    "print('average length of toxic sentences: ', av_len_toxic)\n",
    "print('maximum length of toxic sentences: ', max_len_toxic)\n",
    "\n",
    "print('average length of neutral sentences: ', av_len_neutral)\n",
    "print('maximum length of neutral sentences: ', max_len_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65cf07f3-db00-434e-a58e-970738fc0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(inputs, max_length=max_len_neutral):\n",
    "    model_inputs = tokenizer(inputs['en_toxic_comment'], max_length=max_length, truncation=True)\n",
    "    labels = tokenizer(inputs['en_neutral_comment'], max_length=max_length, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "84a825a7-5b87-409e-8316-e349aebd2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a60b3244-cb5c-4d90-9c61-426e74672147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 13820/13820 [00:03<00:00, 4494.38 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 1975/1975 [00:00<00:00, 3879.28 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 3949/3949 [00:00<00:00, 4087.07 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = detox_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6f6fb4e-6723-4612-a2a6-19cd88b22be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paradetox.evaluation_detox.metric_tools.style_transfer_accuracy import classify_preds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac87d0eb-aa73-4f1d-b500-4ba3f6e85135",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Box({\n",
    "    'batch_size': 64,\n",
    "    'cola_classifier_path': '/content/drive/MyDrive/style_transfer/cola_classifier',\n",
    "    'wieting_tokenizer_path': 'sim.sp.30k.model',\n",
    "    'wieting_model_path': 'sim.pt',\n",
    "    't1': 75., # this is default value\n",
    "    't2': 70., # this is default value\n",
    "    't3': 12. # this is default value\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb0b639b-1545-4775-b599-fa4db05491d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 49.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = ['Fuck off']\n",
    "accuracy_by_sent = classify_preds(args, preds)\n",
    "accuracy = np.mean(accuracy_by_sent)\n",
    "print('\\n')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d921cfc4-3e6d-41b7-a1dd-d18d45568fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import math\n",
    "\n",
    "def calc_gpt_ppl(preds, aggregate=True):\n",
    "    detokenize = lambda x: x.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\").replace(\" )\",\n",
    "                                                                                                                 \")\").replace(\n",
    "        \"( \", \"(\")\n",
    "\n",
    "    print('Calculating token-level perplexity')\n",
    "    gpt_ppl = []\n",
    "\n",
    "    gpt_model = GPT2LMHeadModel.from_pretrained('gpt2-medium').cuda()\n",
    "    gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "    gpt_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sent in tqdm.tqdm(preds):\n",
    "            sent = detokenize(sent)\n",
    "            if len(sent) == 1:\n",
    "                sent = sent + '.'\n",
    "            input_ids = gpt_tokenizer.encode(sent)\n",
    "            inp = torch.tensor(input_ids).unsqueeze(0).cuda()\n",
    "\n",
    "            try:\n",
    "                result = gpt_model(inp, labels=inp, return_dict=True)\n",
    "                loss = result.loss.item()\n",
    "            except Exception as e:\n",
    "                print(f'Got exception \"{e}\" when calculating gpt perplexity for sentence \"{sent}\" ({input_ids})')\n",
    "                loss = 100\n",
    "\n",
    "            gpt_ppl.append(100 if np.isnan(loss) else math.exp(loss))\n",
    "\n",
    "    if aggregate:\n",
    "        return np.mean(gpt_ppl)\n",
    "    return np.array(gpt_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08c49c12-a9dc-40f1-8861-0610531c7f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 55.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.25307249033414"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_gpt_ppl(['what you want?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4df9d958-4f9a-4eba-b6b0-ac6572898a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.6666666666666666,\n",
       "  'p': 0.6666666666666666,\n",
       "  'r': 0.6666666666666666},\n",
       " 'rouge-2': {'f': 0.5, 'p': 0.5, 'r': 0.5},\n",
       " 'rouge-l': {'f': 0.7132754626224419,\n",
       "  'p': 0.7132754626224419,\n",
       "  'r': 0.7132754626224419},\n",
       " 'rouge-w': {'f': 0.5937190915495906,\n",
       "  'p': 0.6666666666666666,\n",
       "  'r': 0.5351610411734872}}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rouge\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                        max_n=2,\n",
    "                        limit_length=True,\n",
    "                        length_limit=100,\n",
    "                        length_limit_type='words',\n",
    "                        apply_avg=True,\n",
    "                        apply_best=False,\n",
    "                        alpha=0.5, \n",
    "                        weight_factor=1.2,\n",
    "                        stemming=True)\n",
    "\n",
    "scores = evaluator.get_scores(['You are idiot'], ['you are stupid'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01a20a48-a352-41db-887d-4c3eda021498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhuld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "84a9516f-6a1a-482d-b6d6-4e0af0919fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated sentences into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference sentences into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    #decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    #decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    #print(decoded_preds)\n",
    "    accuracy_by_sent = classify_preds(args, decoded_preds)\n",
    "    accuracy = np.mean(accuracy_by_sent)\n",
    "    fluency = calc_gpt_ppl(decoded_preds)\n",
    "    rouge_scores = evaluator.get_scores(decoded_preds, decoded_labels)\n",
    "    rouge_scores_dict = {k: round(v['f'], 4) for k, v in rouge_scores.items()}\n",
    "\n",
    "    results = {'STA': accuracy, 'Fluency': fluency}\n",
    "    results.update(rouge_scores_dict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3f1c79e6-3f3c-4211-8702-9a395c3b96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_train_epochs = 10\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "\n",
    "arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"asqiiBART-detox\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=logging_steps,\n",
    "    use_cpu=False,\n",
    "    predict_with_generate=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0a4cb27e-f9ff-4edf-9e19-e668986b36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b539eef9-49ad-4584-bd7f-1bc152b371b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 25:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Rouge-w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.197500</td>\n",
       "      <td>0.970657</td>\n",
       "      <td>0.826835</td>\n",
       "      <td>411.665532</td>\n",
       "      <td>0.802300</td>\n",
       "      <td>0.683600</td>\n",
       "      <td>0.822600</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.950200</td>\n",
       "      <td>0.954055</td>\n",
       "      <td>0.852152</td>\n",
       "      <td>419.786295</td>\n",
       "      <td>0.803900</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>0.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>0.941986</td>\n",
       "      <td>0.892658</td>\n",
       "      <td>415.124272</td>\n",
       "      <td>0.805500</td>\n",
       "      <td>0.686600</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.784600</td>\n",
       "      <td>0.947858</td>\n",
       "      <td>0.904304</td>\n",
       "      <td>454.407348</td>\n",
       "      <td>0.807500</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.826900</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.728200</td>\n",
       "      <td>0.948358</td>\n",
       "      <td>0.900759</td>\n",
       "      <td>505.193413</td>\n",
       "      <td>0.808400</td>\n",
       "      <td>0.690600</td>\n",
       "      <td>0.827800</td>\n",
       "      <td>0.608800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.960254</td>\n",
       "      <td>0.905316</td>\n",
       "      <td>431.799386</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.687600</td>\n",
       "      <td>0.825400</td>\n",
       "      <td>0.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.908861</td>\n",
       "      <td>419.147536</td>\n",
       "      <td>0.805000</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.824800</td>\n",
       "      <td>0.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.620900</td>\n",
       "      <td>0.982985</td>\n",
       "      <td>0.910886</td>\n",
       "      <td>414.437049</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.601700</td>\n",
       "      <td>0.986766</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>424.757781</td>\n",
       "      <td>0.804700</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.824400</td>\n",
       "      <td>0.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.583400</td>\n",
       "      <td>0.991538</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>423.944209</td>\n",
       "      <td>0.802900</td>\n",
       "      <td>0.683900</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>0.604500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 56.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.38it/s]\n",
      "Checkpoint destination directory asqiiBART-detox\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:22<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.66it/s]\n",
      "Checkpoint destination directory asqiiBART-detox\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.763519659307268, metrics={'train_runtime': 1527.5427, 'train_samples_per_second': 90.472, 'train_steps_per_second': 0.707, 'total_flos': 2211178302627840.0, 'train_loss': 0.763519659307268, 'epoch': 10.0})"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b028dde8-df57-478c-a2b7-d444a4ed4499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify(text, max_length=150):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    inputs_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "    output_ids = model.generate(inputs_ids, attention_mask=attention_mask, max_length=150, num_beams=30, early_stopping=False)\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c06180a7-bac4-4b58-a0d7-6bcfb05d0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxics, trues, preds = [], [], []\n",
    "\n",
    "for i in range(len(detox_dataset['test'])):\n",
    "    toxic = detox_dataset['test'][i]['en_toxic_comment']\n",
    "    neutral_true = detox_dataset['test'][i]['en_neutral_comment']\n",
    "    neutral_pred = detoxify(toxic)\n",
    "\n",
    "    toxics.append(toxic)\n",
    "    trues.append(neutral_true)\n",
    "    preds.append(neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "24d27908-3da9-41c9-a16e-0e00ef1e4324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>TrueNeutral</th>\n",
       "      <th>PredictionNeutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anonymous is so fucking annoying .</td>\n",
       "      <td>anonymous is so annoying .</td>\n",
       "      <td>anonymous is so annoying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why would you fucking do anything else ?</td>\n",
       "      <td>Why would you do anything else?</td>\n",
       "      <td>why would you do anything else?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but it doess look good , just fucking shit sca...</td>\n",
       "      <td>but it doess look good , just scarringly good</td>\n",
       "      <td>but it doess look good, just scarringly good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you really been on that wrestling shit again h...</td>\n",
       "      <td>You've really been into wrestling again, huh?</td>\n",
       "      <td>you really been on that wrestling again huh? haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can we atleast listen to gospel if its gonna b...</td>\n",
       "      <td>can we at least listen to gospel if its gonna ...</td>\n",
       "      <td>can we atleast listen to gospel if its gonna b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>i would show u who the bitch is</td>\n",
       "      <td>i would show u who is this</td>\n",
       "      <td>i would show u who she is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>i could make jokes but some one would bitch at...</td>\n",
       "      <td>i could make jokes but some one would think wr...</td>\n",
       "      <td>I could make jokes but some one would make fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>what will it take to fix this shit ?</td>\n",
       "      <td>What will take to fix this problem</td>\n",
       "      <td>What will it take to fix this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>and shit , do you have a cold bro ?</td>\n",
       "      <td>Do you have a cold, bro?</td>\n",
       "      <td>and, do you have a cold bro?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Toxic  \\\n",
       "0                    anonymous is so fucking annoying .   \n",
       "1              why would you fucking do anything else ?   \n",
       "2     but it doess look good , just fucking shit sca...   \n",
       "3     you really been on that wrestling shit again h...   \n",
       "4     can we atleast listen to gospel if its gonna b...   \n",
       "...                                                 ...   \n",
       "3944                    i would show u who the bitch is   \n",
       "3945  i could make jokes but some one would bitch at...   \n",
       "3946               what will it take to fix this shit ?   \n",
       "3947  to say the establishment is fighting tooth and...   \n",
       "3948                and shit , do you have a cold bro ?   \n",
       "\n",
       "                                            TrueNeutral  \\\n",
       "0                            anonymous is so annoying .   \n",
       "1                       Why would you do anything else?   \n",
       "2         but it doess look good , just scarringly good   \n",
       "3         You've really been into wrestling again, huh?   \n",
       "4     can we at least listen to gospel if its gonna ...   \n",
       "...                                                 ...   \n",
       "3944                         i would show u who is this   \n",
       "3945  i could make jokes but some one would think wr...   \n",
       "3946                 What will take to fix this problem   \n",
       "3947  to say the establishment is fighting tooth and...   \n",
       "3948                           Do you have a cold, bro?   \n",
       "\n",
       "                                      PredictionNeutral  \n",
       "0                             anonymous is so annoying.  \n",
       "1                       why would you do anything else?  \n",
       "2          but it doess look good, just scarringly good  \n",
       "3     you really been on that wrestling again huh? haha  \n",
       "4     can we atleast listen to gospel if its gonna b...  \n",
       "...                                                 ...  \n",
       "3944                          i would show u who she is  \n",
       "3945  I could make jokes but some one would make fun...  \n",
       "3946                     What will it take to fix this?  \n",
       "3947  to say the establishment is fighting tooth and...  \n",
       "3948                       and, do you have a cold bro?  \n",
       "\n",
       "[3949 rows x 3 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'Toxic': toxics, 'TrueNeutral': trues, 'PredictionNeutral': preds})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63df2b18-5beb-48d2-8b64-7fceaa8a128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('asqiiBase_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51092e6-0c67-4892-856a-8f75c421a0a1",
   "metadata": {},
   "source": [
    "### Training BART-Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "72efe044-da8f-41ca-883e-5882ad5836db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ebc60adc-5f2d-4731-bd9b-991b104cecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(inputs, max_length=max_len_neutral):\n",
    "    model_inputs = tokenizer(inputs['en_toxic_comment'], max_length=max_length, truncation=True)\n",
    "    labels = tokenizer(inputs['en_neutral_comment'], max_length=max_length, truncation=True)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "\n",
    "    return model_inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "33b0c9d0-1752-4e09-9ddc-6545dbe3fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████| 13820/13820 [00:03<00:00, 4482.98 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 1975/1975 [00:00<00:00, 3871.68 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████| 3949/3949 [00:00<00:00, 4026.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "tokenized_datasets = detox_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2fa1b8f4-7dcc-49df-b0c8-0a4f815047cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_train_epochs = 10\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "\n",
    "arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"asqiiBARTLarge-detox\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=logging_steps,\n",
    "    use_cpu=False,\n",
    "    predict_with_generate=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ae8eca16-da26-48ae-9b15-19deb6d97631",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9aed260f-f67e-429b-bd58-81a4933b3f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 39:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Fluency</th>\n",
       "      <th>Rouge-1</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Rouge-w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.073200</td>\n",
       "      <td>1.015069</td>\n",
       "      <td>0.869367</td>\n",
       "      <td>387.200583</td>\n",
       "      <td>0.801800</td>\n",
       "      <td>0.683100</td>\n",
       "      <td>0.822100</td>\n",
       "      <td>0.605000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.838600</td>\n",
       "      <td>1.030199</td>\n",
       "      <td>0.904304</td>\n",
       "      <td>369.716240</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>0.689200</td>\n",
       "      <td>0.826400</td>\n",
       "      <td>0.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.733100</td>\n",
       "      <td>1.019994</td>\n",
       "      <td>0.924557</td>\n",
       "      <td>752.815062</td>\n",
       "      <td>0.801500</td>\n",
       "      <td>0.682100</td>\n",
       "      <td>0.821600</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.649700</td>\n",
       "      <td>1.063380</td>\n",
       "      <td>0.925570</td>\n",
       "      <td>452.965838</td>\n",
       "      <td>0.804600</td>\n",
       "      <td>0.685600</td>\n",
       "      <td>0.824600</td>\n",
       "      <td>0.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.579200</td>\n",
       "      <td>1.078866</td>\n",
       "      <td>0.927595</td>\n",
       "      <td>494.843183</td>\n",
       "      <td>0.803400</td>\n",
       "      <td>0.682400</td>\n",
       "      <td>0.823700</td>\n",
       "      <td>0.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.523300</td>\n",
       "      <td>1.086039</td>\n",
       "      <td>0.923544</td>\n",
       "      <td>335.290332</td>\n",
       "      <td>0.801000</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.821300</td>\n",
       "      <td>0.602400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>1.121182</td>\n",
       "      <td>0.936709</td>\n",
       "      <td>340.390177</td>\n",
       "      <td>0.797300</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.434100</td>\n",
       "      <td>1.153589</td>\n",
       "      <td>0.930633</td>\n",
       "      <td>444.180831</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.816600</td>\n",
       "      <td>0.598300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.411600</td>\n",
       "      <td>1.168280</td>\n",
       "      <td>0.940253</td>\n",
       "      <td>329.385371</td>\n",
       "      <td>0.794200</td>\n",
       "      <td>0.669300</td>\n",
       "      <td>0.814900</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.386700</td>\n",
       "      <td>1.206725</td>\n",
       "      <td>0.930633</td>\n",
       "      <td>345.636060</td>\n",
       "      <td>0.795200</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>0.815400</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:20<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.77it/s]\n",
      "Checkpoint destination directory asqiiBARTLarge-detox\\checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.46it/s]\n",
      "Checkpoint destination directory asqiiBARTLarge-detox\\checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating style of predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:21<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating token-level perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1975/1975 [00:34<00:00, 57.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.608669744376783, metrics={'train_runtime': 2352.326, 'train_samples_per_second': 58.75, 'train_steps_per_second': 0.459, 'total_flos': 7856406007087104.0, 'train_loss': 0.608669744376783, 'epoch': 10.0})"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aab16dc9-b59a-482b-a158-a8e03169c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detoxify(text, max_length=150):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    inputs = inputs.to(device)\n",
    "    inputs_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "    output_ids = model.generate(inputs_ids, attention_mask=attention_mask, max_length=150, num_beams=30, early_stopping=False)\n",
    "    output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5da2ebc1-6599-4b14-8331-34ac7af47a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxics, trues, preds = [], [], []\n",
    "\n",
    "for i in range(len(detox_dataset['test'])):\n",
    "    toxic = detox_dataset['test'][i]['en_toxic_comment']\n",
    "    neutral_true = detox_dataset['test'][i]['en_neutral_comment']\n",
    "    neutral_pred = detoxify(toxic)\n",
    "\n",
    "    toxics.append(toxic)\n",
    "    trues.append(neutral_true)\n",
    "    preds.append(neutral_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "831ef1ca-3f7a-41c9-8a3f-55c25dc07e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxic</th>\n",
       "      <th>TrueNeutral</th>\n",
       "      <th>PredictionNeutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anonymous is so fucking annoying .</td>\n",
       "      <td>anonymous is so annoying .</td>\n",
       "      <td>anonymous is so annoying.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why would you fucking do anything else ?</td>\n",
       "      <td>Why would you do anything else?</td>\n",
       "      <td>Why would you do anything else?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but it doess look good , just fucking shit sca...</td>\n",
       "      <td>but it doess look good , just scarringly good</td>\n",
       "      <td>but it doess look good, just scarringly good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you really been on that wrestling shit again h...</td>\n",
       "      <td>You've really been into wrestling again, huh?</td>\n",
       "      <td>you really been on that wrestling again huh? haha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can we atleast listen to gospel if its gonna b...</td>\n",
       "      <td>can we at least listen to gospel if its gonna ...</td>\n",
       "      <td>can we atleast listen to gospel if its gonna b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3944</th>\n",
       "      <td>i would show u who the bitch is</td>\n",
       "      <td>i would show u who is this</td>\n",
       "      <td>i would show u who she is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>i could make jokes but some one would bitch at...</td>\n",
       "      <td>i could make jokes but some one would think wr...</td>\n",
       "      <td>i could make jokes but some one would yell at me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3946</th>\n",
       "      <td>what will it take to fix this shit ?</td>\n",
       "      <td>What will take to fix this problem</td>\n",
       "      <td>What will it take to fix this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3947</th>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "      <td>to say the establishment is fighting tooth and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>and shit , do you have a cold bro ?</td>\n",
       "      <td>Do you have a cold, bro?</td>\n",
       "      <td>Do you have a cold bro?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Toxic  \\\n",
       "0                    anonymous is so fucking annoying .   \n",
       "1              why would you fucking do anything else ?   \n",
       "2     but it doess look good , just fucking shit sca...   \n",
       "3     you really been on that wrestling shit again h...   \n",
       "4     can we atleast listen to gospel if its gonna b...   \n",
       "...                                                 ...   \n",
       "3944                    i would show u who the bitch is   \n",
       "3945  i could make jokes but some one would bitch at...   \n",
       "3946               what will it take to fix this shit ?   \n",
       "3947  to say the establishment is fighting tooth and...   \n",
       "3948                and shit , do you have a cold bro ?   \n",
       "\n",
       "                                            TrueNeutral  \\\n",
       "0                            anonymous is so annoying .   \n",
       "1                       Why would you do anything else?   \n",
       "2         but it doess look good , just scarringly good   \n",
       "3         You've really been into wrestling again, huh?   \n",
       "4     can we at least listen to gospel if its gonna ...   \n",
       "...                                                 ...   \n",
       "3944                         i would show u who is this   \n",
       "3945  i could make jokes but some one would think wr...   \n",
       "3946                 What will take to fix this problem   \n",
       "3947  to say the establishment is fighting tooth and...   \n",
       "3948                           Do you have a cold, bro?   \n",
       "\n",
       "                                      PredictionNeutral  \n",
       "0                             anonymous is so annoying.  \n",
       "1                       Why would you do anything else?  \n",
       "2          but it doess look good, just scarringly good  \n",
       "3     you really been on that wrestling again huh? haha  \n",
       "4     can we atleast listen to gospel if its gonna b...  \n",
       "...                                                 ...  \n",
       "3944                          i would show u who she is  \n",
       "3945  i could make jokes but some one would yell at me.  \n",
       "3946                     What will it take to fix this?  \n",
       "3947  to say the establishment is fighting tooth and...  \n",
       "3948                            Do you have a cold bro?  \n",
       "\n",
       "[3949 rows x 3 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.DataFrame({'Toxic': toxics, 'TrueNeutral': trues, 'PredictionNeutral': preds})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2a077809-5519-4931-895a-86f54f3444e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('asqiiLarge_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b7d54dbe-0477-4ed8-b34c-6dabf6127847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This person should work at the cafe'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('This idiot should work at the cafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d884450d-975a-44e7-ad13-50f94f8a62da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'She should work at the cafe'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('This bitch should work at the cafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "332ed540-861e-4ab7-9c06-b86c2842ad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I wanna kill all of you'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('I wanna kill yall morons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0528cac3-6a2d-459e-af54-131eadb84e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'howdy fellas, whats good'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('howdy fellas, whats good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9bfc72a4-190b-4213-9dd2-8032cb814931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('You are not a good person.', 'you are an iddoijd')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify(\"You are an idoit\"), detoxify('You are an iddoijd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c4d4a86d-4f78-4243-af03-d68e890ae472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to take a drink now'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('I want to take a shit now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2839987d-72c6-46d4-8e4a-3a6b6fb57f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to hit you'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('I want to smack your faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "405afee1-6614-4b0c-ba79-5b5adaf59d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Hahaha.', \"I don't care.\")"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detoxify('tchknt, haha'), detoxify('tchknt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65959c00-c760-4ce8-8ad8-eef2e11c1a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
